# -*- coding: utf-8 -*-
"""Predictive Analytics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Mnllx1mAmHojn9yQpOw_Kl6feSCo1IQO
"""

# Import library yang akan digunakan
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score

# Memuat dataset
dataset = pd.read_csv('https://raw.githubusercontent.com/hansk88/Proyek-1-Predictive-Analysis/refs/heads/main/openweatherdata-denpasar-1990-2020.csv')

# Mencetak lima baris pertama dari dataset
dataset.head()

# Memeriksa info pada dataset
dataset.info()

"""Terdapat 32 kolom dengan rincian sebagai berikut:
* 21 kolom bertipe float
* 6 kolom bertipe integer
* 5 kolom bertipe object

Selain itu diketahui juga bahwa jumlah data yaitu sebanyak 264,924 sampel data

Perlu diperhatikan bahwa dalam proyek kali ini, ada dua kolom yang dapat menjadi label dataset yaitu kolom **weather_main** dan **weather_description**. Karena kolom **weather_description** lebih lengkap, maka kolom ini akan dijadikan sebagai label dataset. Selanjutnya, dataset yang sudah ada akan dibersihkan terlebih dahulu
"""

# Menghapus kolom yang tidak relevan
dataset.drop(['dt', 'dt_iso', 'timezone', 'city_name', 'lat', 'lon', 'weather_id', 'weather_main', 'weather_icon'], inplace=True, axis=1)

"""Dari info sebelumnya, didapati bahwa kolom **sea_level**, **grnd_level**, **rain_today**, **snow_1h**, **snow_3h**, **snow_6h**, **snow_12h**, **snow_24h**, **snow_today** tidak memiliki nilai sama sekali sehingga akan dihapus terlebih dahulu"""

# Menghapus kolom yang nilainya null semua
dataset.drop(['sea_level', 'grnd_level', 'rain_today', 'snow_1h', 'snow_3h', 'snow_6h', 'snow_12h', 'snow_24h', 'snow_today'], inplace=True, axis=1)

"""Dapat kita lihat juga bahwa antara kolom **temp** dengan kolom **temp_min** dan kolom **temp_max** cenderung memiliki nilai yang sama. Untuk selanjutnya, akan dicek apakah hal tersebut memang benar. Jika benar, maka kolom **temp_min** dan **temp_max** akan dihapus"""

# Menghitung baris dimana kolom temp, temp_min, dan temp_max memiliki setidaknya satu nilai yang berbeda
check_diff = ((dataset["temp"] != dataset["temp_min"]) &
              (dataset["temp"] != dataset["temp_max"])).sum()


print(f"Jumlah baris dimana nilai antara ketiga kolom memiliki setidaknya satu perbedaan: {check_diff}")

"""Setelah di-run, diketahui bahwa nilai antar kolom tersebut ternyata ada yang berbeda sehingga kolom **temp_min** dan **temp_max** tidak jadi dihapus"""

# Memeriksa kolom yang memiliki nilai null
dataset.isna().sum()

"""Dapat dilihat bahwa sekitar 86.29% (atau lebih) data dari kolom **rain_1h**, **rain_3h**, **rain_6h**, **rain_12h**, **rain_24h** adalah null dan hasilnya akan bias semisal nilai yang null tersebut diisi dengan nilai mean, modus, ataupun nilai lainnya sehingga kolom-kolom tersebut akan dihapus juga"""

# Menghapus kolom yang nilainya mayoritas null semua
dataset.drop(['rain_1h', 'rain_3h', 'rain_6h', 'rain_12h', 'rain_24h'], inplace=True, axis=1)

# Memeriksa ulang info pada dataset
dataset.info()

# Memeriksa duplikasi pada dataset
data_duplikat = dataset.duplicated().sum()
print("Jumlah data duplikat: ", data_duplikat)

# Menghapus data duplikat
dataset.drop_duplicates(inplace=True)
dataset.shape

# Memeriksa statistik deskriptif pada dataset
dataset.describe()

"""Diperoleh nilai mean, median, Q1, Q2, Q3, min, serta max untuk setiap kolom bertipe float maupun integer"""

# Mendeteksi outlier
num_features = ['temp', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'clouds_all']
plt.figure(figsize=(14, 10))
for i, column in enumerate(num_features, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(x=dataset[column])
    plt.title(f'Distribustion of {column}')
plt.tight_layout()
plt.show()

# Menentukan Q1, Q3, dan IQR untuk masing-masing kolom
Q1 = dataset[num_features].quantile(0.25)
Q3 = dataset[num_features].quantile(0.75)
IQR = Q3 - Q1

# Menghapus baris yang mengandung outlier
filter_outliers = ~((dataset[num_features] < (Q1 - 1.5 * IQR)) |
                    (dataset[num_features] > (Q3 + 1.5 * IQR))).any(axis=1)
dataset = dataset[filter_outliers]

# Cek ukuran dataset setelah outlier dihapus
dataset.shape

"""Setelah outlier pada masing-masing kolom dihapus didapati sampel data berkurang sebanyak 55,154 atau 23.67%"""

# Menghitung jumlah sampel data pada masing-masing label
dataset['weather_description'].value_counts()

# Membuat diagram batang untuk menunjukkan distribusi entri pada kolom label
counts = dataset['weather_description'].value_counts()
counts.plot(kind='bar', figsize=(10,6))
plt.title('Distribusi Cuaca')
plt.xlabel('Cuaca')
plt.ylabel('Jumlah')
plt.show()

"""Selanjutnya kita akan melihat korelasi setiap fitur dengan label. Karena label masih berbentuk object maka dilakukan encoding untuk merubahnya menjadi angka (numerik)"""

# Encoding kolom label
label_encoders = {}

categorical_columns = dataset.select_dtypes(include=['object']).columns

for column in categorical_columns:
    le = LabelEncoder()
    dataset[column] = le.fit_transform(dataset[column])
    label_encoders[column] = le

# Menampilkan matriks korelasi untuk melihat korelasi antar kolom fitur dengan kolom label
num_features = dataset.select_dtypes(include=[np.number])
plt.figure(figsize=(12, 10))
correlation_matrix = num_features.corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

"""Didapati bahwa korelasi antar kolom fitur dengan kolom label cenderung lemah, dengan pengecualian untuk kolom **clouds_all**"""

# Melakukan inverse untuk kolom weather_description yang sebelumnya di-encode
for column, encoder in label_encoders.items():
    dataset[column] = encoder.inverse_transform(dataset[column])

# Splitting data menjadi data latih dan data uji
X = dataset.drop(["weather_description"],axis =1)
y = dataset["weather_description"]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)

print(f'Total data latih: {len(X_train)}')
print(f'Total data uji: {len(X_test)}')

# Pelatihan Model
knn = KNeighborsClassifier().fit(X_train, y_train)
rf = RandomForestClassifier().fit(X_train, y_train)
boosting = AdaBoostClassifier().fit(X_train, y_train)

# Fungsi untuk mengevaluasi dan mengembalikan hasil sebagai sebuah dictionary
def evaluate_model(model, X_test, y_test):
    y_pred = model.predict(X_test)

    results = {
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted'),
        'Recall': recall_score(y_test, y_pred, average='weighted'),
        'F1-Score': f1_score(y_test, y_pred, average='weighted')
    }
    return results

# Mengevaluasi setiap model dan mengumpulkan hasilnya
results = {
    'K-Nearest Neighbors (KNN)': evaluate_model(knn, X_test, y_test),
    'Random Forest (RF)': evaluate_model(rf, X_test, y_test),
    'AdaBoost (Boosting)': evaluate_model(boosting, X_test, y_test)
}

# Membuat dataframe untuk hasil evaluasi
summary = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score'])

rows = []
for model_name, metrics in results.items():
    rows.append({
        'Model': model_name,
        'Accuracy': metrics['Accuracy'],
        'F1-Score': metrics['F1-Score'],
        'Precision': metrics['Precision'],
        'Recall': metrics['Recall']
    })

summary = pd.DataFrame(rows)

# Menampilkan hasil evaluasi
summary

"""# Inferensi model"""

# Memasukkan input
temp = float(input("temp = "))
temp_min = float(input("temp_min = "))
temp_max = float(input("temp_max = "))
pressure = float(input("pressure = "))
humidity = float(input("humidity = "))
wind_speed = float(input("wind_speed = "))
wind_deg = float(input("wind_deg = "))
clouds_all = float(input("clouds_all = "))

# Membuat semua input menjadi sebuah dataframe
input_data = pd.DataFrame([{
    "temp": temp,
    "temp_min": temp_min,
    "temp_max": temp_max,
    "pressure": pressure,
    "humidity": humidity,
    "wind_speed": wind_speed,
    "wind_deg": wind_deg,
    "clouds_all": clouds_all
}])

# Memprediksi cuaca
predicted_label = boosting.predict(input_data)[0]
print("Predicted weather: ", predicted_label)